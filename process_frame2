def process_frame(self, frame, frame_number=0):
        """í”„ë ˆì„ë³„ ì²˜ë¦¬"""
        results = {
            'frame_number': frame_number,
            'persons': [],
            'total_emotions': defaultdict(int)
        }

        # YOLOë¡œ ì‚¬ëŒ íƒì§€
        yolo_results = self.yolo(frame, classes=[0], verbose=False)  # class 0 = person

        if yolo_results[0].boxes is not None:
            for i, box in enumerate(yolo_results[0].boxes):
                confidence = float(box.conf[0])
                if confidence < 0.5:  # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ìŠ¤í‚µ
                    continue

                # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                x1, y1, x2, y2 = map(int, box.xyxy[0])

                # ì‚¬ëŒ ì˜ì—­ ìë¥´ê¸°
                person_crop = frame[y1:y2, x1:x2]

                if person_crop.size == 0:
                    continue

                # ì–¼êµ´ ê²€ì¶œ
                faces = self.detect_faces_in_person(person_crop)

                person_data = {
                    'person_id': i,
                    'bbox': [x1, y1, x2, y2],
                    'yolo_confidence': confidence,
                    'faces': [],
                    'dominant_emotion': 'neutral',
                    'emotion_confidence': 0.0
                }

                # ê° ì–¼êµ´ì— ëŒ€í•´ ê°ì • ë¶„ì„
                if len(faces) > 0:
                    face_emotions = []

                    for (fx, fy, fw, fh) in faces:
                        # ì ˆëŒ€ ì¢Œí‘œë¡œ ë³€í™˜
                        abs_fx, abs_fy = x1 + fx, y1 + fy
                        abs_fx2, abs_fy2 = abs_fx + fw, abs_fy + fh

                        # ì–¼êµ´ ì˜ì—­ ìë¥´ê¸°
                        face_crop = frame[abs_fy:abs_fy2, abs_fx:abs_fx2]

                        if face_crop.size == 0:
                            continue

                        # ê°ì • ë¶„ì„
                        emotion, emotion_conf, emotion_probs = self.emotion_analyzer.predict_emotion(face_crop)

                        face_data = {
                            'bbox': [abs_fx, abs_fy, abs_fx2, abs_fy2],
                            'emotion': emotion,
                            'confidence': emotion_conf,
                            'probabilities': emotion_probs
                        }

                        person_data['faces'].append(face_data)
                        face_emotions.append((emotion, emotion_conf))

                    # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ê°ì •ì„ ì‚¬ëŒì˜ ëŒ€í‘œ ê°ì •ìœ¼ë¡œ
                    if face_emotions:
                        best_emotion = max(face_emotions, key=lambda x: x[1])
                        person_data['dominant_emotion'] = best_emotion[0]
                        person_data['emotion_confidence'] = best_emotion[1]

                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.person_emotions[i].append(best_emotion[0])
                        results['total_emotions'][best_emotion[0]] += 1

                results['persons'].append(person_data)

        self.frame_stats.append(results)
        return results

    def draw_annotations(self, frame, frame_results):
        """ê²°ê³¼ë¥¼ í”„ë ˆì„ì— ê·¸ë¦¬ê¸°"""
        annotated_frame = frame.copy()

        for person in frame_results['persons']:
            x1, y1, x2, y2 = person['bbox']
            emotion = person['dominant_emotion']
            emotion_conf = person['emotion_confidence']

            # ê°ì •ì— ë”°ë¥¸ ìƒ‰ìƒ
            color = self.emotion_analyzer.emotion_colors.get(emotion, (255, 255, 255))

            # ì‚¬ëŒ ë°”ìš´ë”© ë°•ìŠ¤
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

            # ê°ì • ë¼ë²¨
            if emotion_conf > 0:
                label = f"{emotion}: {emotion_conf:.2f}"
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]

                # ë¼ë²¨ ë°°ê²½
                cv2.rectangle(annotated_frame, (x1, y1 - label_size[1] - 10),
                              (x1 + label_size[0], y1), color, -1)

                # ë¼ë²¨ í…ìŠ¤íŠ¸
                cv2.putText(annotated_frame, label, (x1, y1 - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)

            # ì–¼êµ´ ë°”ìš´ë”© ë°•ìŠ¤
            for face in person['faces']:
                fx1, fy1, fx2, fy2 = face['bbox']
                cv2.rectangle(annotated_frame, (fx1, fy1), (fx2, fy2), color, 1)

        # ì „ì²´ í†µê³„ í‘œì‹œ
        y_offset = 30
        for emotion, count in frame_results['total_emotions'].items():
            if count > 0:
                text = f"{emotion}: {count}"
                cv2.putText(annotated_frame, text, (10, y_offset),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                y_offset += 25

        return annotated_frame

    def process_video(self, input_path, output_path=None, save_stats=True):
        """ë¹„ë””ì˜¤ ì „ì²´ ì²˜ë¦¬"""
        cap = cv2.VideoCapture(input_path)

        if not cap.isOpened():
            print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
            return None

        # ë¹„ë””ì˜¤ ì •ë³´
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì •ë³´: {width}x{height}, {fps:.1f}FPS, {total_frames}í”„ë ˆì„")

        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
        out = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        frame_number = 0
        start_time = time.time()

        print("ğŸ¬ ê°ì •ë¶„ì„ ì²˜ë¦¬ ì‹œì‘...")

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # í”„ë ˆì„ ì²˜ë¦¬
                frame_results = self.process_frame(frame, frame_number)

                # ê²°ê³¼ ê·¸ë¦¬ê¸°
                annotated_frame = self.draw_annotations(frame, frame_results)

                # ì§„í–‰ë¥  í‘œì‹œ
                if frame_number % 30 == 0:
                    progress = (frame_number / total_frames) * 100
                    elapsed = time.time() - start_time
                    fps_processing = frame_number / elapsed if elapsed > 0 else 0
                    print(f"ì§„í–‰ë¥ : {progress:.1f}% ({frame_number}/{total_frames}) - {fps_processing:.1f} FPS")

                # ì¶œë ¥ ë¹„ë””ì˜¤ì— ì €ì¥
                if out:
                    out.write(annotated_frame)

                frame_number += 1

            processing_time = time.time() - start_time
            print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ! ì´ ì‹œê°„: {processing_time:.1f}ì´ˆ")

            # í†µê³„ ì €ì¥
            if save_stats:
                try:
                    self.save_analysis_results(input_path, output_path)
                except Exception as e:
                    print(f"âš ï¸ í†µê³„ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                    print("ğŸ“Š ê¸°ë³¸ í†µê³„ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤:")
                    self.print_basic_stats()

            return self.frame_stats

        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
        finally:
            cap.release()
            if out:
                out.release()
